---
layout: post
title: Distinguishing Homophenes using Multi-head
Visual-audio Memory for Lip Reading
date: 2022-03-11 00:10:00 +0900
category: lab-paper
use_math: true
comments: true
---


본 포스팅은 아래 게시물을 참고했습니다.

- <https://www.aaai.org/AAAI22Papers/AAAI-7528.ParkS.pdf>

---

연구실의 논문을 읽어보았습니다.

## Abstract

- Lip reading 어려운 점
  - information insufficiency
  - homophenes
- Multi-head Visual-audio Memory(MVM)
  - audio-visual representation의 inter-relationship을 모델링해서 audio-visual dataset을 학습하고, audio representation을 기억한다.
  - multi-head key memory에 visual feature를 저장하고, value memory에 audio knowledge를 저장한다.
  - 가능한 audio feature를 memory로 부터 얻는다.
  - multi-temporal level을 사용하여 context에 대해 고려해서 homophenes를 구별하여 audio feature를 얻게한다.

---

## 1. Introduction

- audio-based Automatic Speech Recogntion(ASR)의 어려움 
  - information insufficiency, homophenes
- deep memory network
  - stored knowledge를 이용하여 다양한 application에 사용
  - cross-modal memory network
    - information limited 상황에서 유익
    - homophenes 때문에 lip reading에 바로 사용하기는 어려웠다.
- Multi-head Visual-audio memory(MVM)
  - multi-head key memories & one value memory
  - multi-head attention처럼 가능한 audio representation을 model이 jointly하게 생각하도록 한다.
  - multi-temporal level를 사용하여 context를 생각하면서 audio feature를 추출하도록 한다.
- Effectiveness 증명
  - homophenes에 속하는 words pairs들을 검사
  - 다른 head key memories의 addressing score를 visualize
    - homophene $\Rightarrow$ Multi-head Visual-audio Memory(MVM)으로 candidate audio representation을 추출
    - Memory in multi-temporal levels을 이용하여 context를 이용
    - word-level lip reading에서 sota

---

## 2. Related work

### 2.1 Lip Reading

- architecture
  - better spatio-temporal features
  - word-level data를 포함하는 LRW로 학습
  - 3D convolution layer와 2D ResNet으로 front-end + LSTM으로 back-end architecture
  - 2 stream network (raw video, optical flow)
  - Multi-Scale Temporal Convolution Network(MS-TCN)으로 temporal encoding을 향상시킴, word-level lip reading 성능을 높임
  - sentence-level VSR
  - Connectionist Temporal Classification(CTC) loss function
  - English audio-visual corpus dataset과 Seq2Seq-based architecture
  - Transformer 사용한 sentence-level lip reading 성능 향상
- audio information을 활용
  - pretrained ASR model을 사용하여 unlabeled data를 활용했다.
  - teacher-student framework
    - pretrained ASR model이 teacher
    - trained the lip reading model이 student
    - knowledge distillation을 사용하였다.
  - self-supervised learning method
    - pre-trained visual front-end : lip video로 acoustic feature를 추측
  - cross-modal memory network
  - cross-modal memory network 한계
- 이 논문
  - cross-modal memory network : visual and audio representation을 따로 저장
  - multi-head key memory로 homophenes 문제 해소
  - multi-temporal levels을 적용하여 context를 이해

### 2.2 Memory Network

- Memory Network는 sequential data modelling에서 정보를 잊는 문제를 완화하기 위해 사용되었다.
- cross-modal memory로도 사용
- 

---

## 3. Methods

---

## 4. Experiment

---

## 5. Conclusion