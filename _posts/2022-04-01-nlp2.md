---
layout: post
title: NLP - Count based word Representation
date: 2022-04-01 00:10:00 +0900
category: nlp
use_math: true
comments: true
---

본 포스팅은 아래 위키독을 참고하여 작성되었습니다.

<https://wikidocs.net/book/2155>

## Count based word Representation

### 1. 다양한 단어의 표현 방법

- Local Representation, Discrete Representation : 해당 단어 그 자체만 보고 단어 표현
- Distributed Representation, Continuous Represnetation : 주변을 참고하여 단어 표현

- 잠재 의미 분석(LSA)이나 잠재 디리클레 할당(LDA)과 같은 방법들은 단어의 의미를 표현할 수 있다는 점에서 연속 표현(Continuous Represnetation)이지만, 엄밀히 말해서 다른 접근의 방법론을 사용하고 있는 워드투벡터(Word2vec)와 같은 분산 표현(Distributed Representation)은 아닌 것으로 분류 (?)

### 2. bag of words(bow)

### 3. document-term matrix(dtm) , TDM

한계

1. Sparse representation : sparse vector, sparse matrix
2. 단순 빈도 수 기반 접근 : the는 어디서나 많이나온다.

### 4. td-idf(Term Frequency-Inverse Document Frequency)

- tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수
- df(t) : 특정 단어 t가 등장한 문서의 수
- idf(d, t) : df(t)에 반비례하는 수. $log(\frac{n}{1+df(t)})$
- 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다.

### 5. 문장의 유사도

- Jaccard similarity
  - 교집합의 원소 / 합집합의 원소
- 레벤슈타인 거리
  - 두개의 문장을 2차원 배열로 나타내어서 각 문장의 각 글자들을 비교하여 몇개의 글자가 다른지 구분하는 방법