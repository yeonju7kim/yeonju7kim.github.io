---
layout: post
title: (AI502) 1.ml review
date: 2022-03-11 00:10:00 +0900
category: class-summary
use_math: true
comments: true
---

본 포스팅은 AI502 수업에서 제가 새로 알게 된 부분만 정리한 것입니다.

---

## Bias와 Variance

Bias

> - $Bias[h(x)]=\bar{h(x)}-f(x)$
> - Bias는 추정값의 평균과 실제 값의 차이
> - Bias가 생기는 이유
>   - decision boundary를 표현할 수 없음.
>   - 잘못된 추정
>   - model이 global, smooth함.
>   - underfitting

Variance

> - $Var[h(x)]=E[(h(x)-\bar{h(x)})^2]$
> - 분산
> - Variance가 생기는 이유
>   - local
>   - sharp decision boundary
>   - randomization(학습 처음 시작할 때)
>   - overfitting

Bias-Variance Decomposition

$E[(y-h(x))^2]=E[(h(x)-\bar{h(x)})^2]+(\bar{h(x)}-f(x))^2+E[(y-f(x))^2]$

Error = Variance + Bias^2 + Noise

## Bagging

다른 sample들로 학습한 여러 모델들로 추론하고 평균내는 방법

## Regularization

bias를 증가시키고, variance를 줄이는 방법

## Singular Value Decomposition(SVD)

$A=U\Sigma V^T$, U와 V는 orthogonal, $\Sigma$는 singular value로 구성된 diagonal matrix

## Multivariate Linear regression

- $\theta^{OLS}=min_{\theta}\lVert y-\theta\rVert_2^2=<y-X\theta,y-X\theta>=(y-X\theta)^T(y-X\theta)$

- $\theta=(X^TX)^{-1}X^Ty$ : normal equation

- $X^TX$ : normal matrix

## Probabilistic interpretation

likelihood function : 주어진 X,$\theta$에 대한 yi의 분포

- ![alt image](/public/img/220311/likelihood_function.png)
- ![alt image](/public/img/220311/likelihood_function2.png)
- probabilistic model에서 likelihood를 maximize하는 $\theta$를 찾아야한다.
- 2번째 식을 log likelihood로 바꿔서 식을 전개하다 보면 minimizing RSS로 바뀐다.

## Ridge Regression

- L2-regularization을 쓰는 Linear regression
- ![alt image](/public/img/220311/ridge_regression.png)
- $\lambda$가 0이면 $\hat{\theta}^{ridge}=\hat{\theta}^{OLS}$
- PRSS라고도 표현한다.

## Logistic Regression
