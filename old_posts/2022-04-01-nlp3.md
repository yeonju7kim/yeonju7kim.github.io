---
layout: post
title: NLP - ML DL
date: 2022-04-01 00:10:00 +0900
category: nlp
use_math: true
comments: true
---

본 포스팅은 아래 위키독을 참고하여 작성되었습니다.

<https://wikidocs.net/book/2155>

---

- 자기지도 학습(Self-Supervised Learning, SSL)
  - Word2Vec과 같은 워드 임베딩 알고리즘이나, BERT와 같은 언어 모델

- precision : true로 추론한 것 중 실제 true
- recall : 맞춘 것 중 true
- accuracy : 맞춘 것의 비율

## 워드 임베딩

- Dense Representation
- LSA, Word2Vec, FastText, Glove 
- 분산 표현
  - 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다
- CBOW : 중간에 있는 단어들을 예측
- Skip-Gram : 주변 단어들을 예측하는 방법
- NNLM Vs. Word2Vec
  - NNLM : 다음 단어 예측
  - Word2Vec : 중심 단어를 예측

## 영어/한국어 Word2Vec 실습

- gensim

``` python
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)

model.wv.save_word2vec_format('eng_w2v') # 모델 저장
loaded_model = KeyedVectors.load_word2vec_format("eng_w2v") # 모델 로드

print(model.wv.most_similar("최민식"))

```

## Skip-Gram with Negative Sampling, SGNS

- 네거티브 샘플링 : Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법
- 한 단어의 중심 단어들을 같이 입력으로 넣고 1로 라벨링, 랜덤으로 선택된 다른 단어는 0으로 라벨링

1. 중심 단어를 위한 임베딩, 주변 단어를 위한 임베딩
2. Dot -> Activation 해서 결과 만듦
3. 이렇게 임베딩해주는 모델 학습

## GloVe

- 카운트 기반과 예측 기반을 모두 사용하는 방법론
- 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완한다는 목적
  - LSD : 빈도수 카운트하고, 차원을 축소하여 잠재된 의미 끌어내는 방법
  - Word2Vec : 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론
- 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것

```python
from glove import Corpus, Glove

corpus = Corpus() 

# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성
corpus.fit(result, window=5)
glove = Glove(no_components=100, learning_rate=0.05)

# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
print(glove.most_similar("man"))
```

## FastText

- 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주
- <ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple> 로 벡터값을 얻어서 word2Vec를 함.
- 강점
  - OOV 대응 
    - Word2Vec는 학습 데이터에 존재하지 않는 단어. 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없습니다.
  - Rare Word 대응

## 사전 훈련된 워드 임베딩

